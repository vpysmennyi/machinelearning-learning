{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hometask7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1C6Tk-Z-I-Lxu0s0QO-Qy02jDcVQw7uB9",
      "authorship_tag": "ABX9TyOz7Oxa567Z3Z+WU3oS073+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vpysmennyi/machinelearning-learning/blob/main/Hometask7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUhDjeompKlu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c2698ff-3267-47c5-e5f8-29e21d4f0ef2"
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import STL10\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "class Print(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Print, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(x.shape)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Logger():\n",
        "    def __init__(self, file = './CNN_execution.log'):\n",
        "        self.logf = open(file, 'w')\n",
        "\n",
        "    def write(self, text):\n",
        "        self.logf.write(self.sysdate() + ' : ' + text + '\\n')\n",
        "        self.logf.flush()\n",
        "\n",
        "    def close(self):\n",
        "        self.logf.close()\n",
        "\n",
        "    def sysdate(self):\n",
        "        return datetime.now().strftime('%y%m%d %H:%M:%S')\n",
        "\n",
        "\n",
        "class DAE(nn.Module):\n",
        "    def __init__(self, pretrain=False):\n",
        "        super(DAE, self).__init__()\n",
        "\n",
        "        self.pretrain = pretrain\n",
        "\n",
        "        self.enc = nn.Sequential(\n",
        "            # 3x96x96\n",
        "            self.convBlock(3, 64, kernel=(3, 3), padding=1),\n",
        "            nn.MaxPool2d(3, 3),\n",
        "            # 64x32x32\n",
        "            self.convBlock(64, 64, (3, 3), 1),\n",
        "            self.convBlock(64, 32, (3, 3), 1),\n",
        "            self.convBlock(32, 16, (3, 3), 1),\n",
        "            # 16x32x32\n",
        "            nn.MaxPool2d(4, 4),\n",
        "            # 16x8x8\n",
        "\n",
        "            self.convBlock(16, 8, (3, 3), 1)\n",
        "\n",
        "            # 8x8x8\n",
        "        )\n",
        "\n",
        "        self.dec = nn.Sequential(\n",
        "            self.deconvBlock(8, 16, (3, 3), 1),\n",
        "            nn.UpsamplingBilinear2d(scale_factor=4),\n",
        "\n",
        "            self.deconvBlock(16, 32, (3, 3), 1),\n",
        "            self.deconvBlock(32, 64, (3, 3), 1),\n",
        "            self.deconvBlock(64, 64, (3, 3), 1),\n",
        "\n",
        "            nn.UpsamplingBilinear2d(scale_factor=3),\n",
        "            self.deconvBlock(64, 3, (3, 3), 1)\n",
        "        )\n",
        "\n",
        "        self.cl = nn.Sequential(nn.Linear(512, 256),\n",
        "                           nn.ReLU(),\n",
        "                           nn.BatchNorm1d(256),\n",
        "                           nn.Linear(256,128),\n",
        "                           nn.ReLU(),\n",
        "                           nn.BatchNorm1d(128),\n",
        "                           nn.Linear(128,10)\n",
        "                           )\n",
        "\n",
        "        if pretrain:\n",
        "            self.net = nn.Sequential(self.enc, self.dec)\n",
        "        else:\n",
        "            self.net = nn.Sequential(self.enc, nn.Flatten(), self.cl)\n",
        "\n",
        "\n",
        "    def convBlock(self, input, output, kernel, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(input, output, kernel_size=kernel, padding=padding),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(output))\n",
        "\n",
        "    def deconvBlock(self, input, output, kernel, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(input, output, kernel_size=kernel, padding=padding),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(output))\n",
        "        \n",
        "    def init_cl_weights(self, m):\n",
        "         if type(m) == nn.Linear:\n",
        "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def train(dataset, network, loss_fn, opt):\n",
        "    e_acc, e_loss = [], []\n",
        "\n",
        "    network.train()\n",
        "    for (X, Y) in dataset:\n",
        "        opt.zero_grad()\n",
        "\n",
        "        Yp = network(X)\n",
        "        loss = loss_fn(Yp, Y)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        acc = (Yp.argmax(1) == Y).sum() / len(Y)\n",
        "\n",
        "        e_loss.append(loss.detach().cpu())\n",
        "        if not network.pretrain:\n",
        "            e_acc.append(acc)\n",
        "\n",
        "    e_loss = torch.stack(e_loss).mean()\n",
        "\n",
        "    if not network.pretrain:\n",
        "        e_acc = torch.stack(e_acc).mean()\n",
        "    \n",
        "    return e_acc, e_loss\n",
        "\n",
        "def validate(dataset, network, loss_fn):\n",
        "    val_loss, val_acc = [],[]\n",
        "\n",
        "    network.eval()\n",
        "    for (X, Y) in dataset:\n",
        "        Yp = network(X)\n",
        "\n",
        "        loss = loss_fn(Yp, Y)\n",
        "        acc = (Yp.argmax(1) == Y).sum() / len(Y)\n",
        "\n",
        "        val_acc.append(acc)\n",
        "        val_loss.append(loss.detach().cpu())\n",
        "\n",
        "    val_loss = torch.stack(val_loss).mean()\n",
        "    val_acc = torch.stack(val_acc).mean()\n",
        "\n",
        "    return val_loss, val_acc\n",
        "\n",
        "batch_size = 32\n",
        "lr = 1e-3\n",
        "pretrain_epochs = 10\n",
        "num_epochs = 40\n",
        "noise_factor = 0.3\n",
        "\n",
        "train_ds = STL10('.', split='train', folds=None, transform=transforms.ToTensor(), download=True)\n",
        "X = torch.stack([train_ds[i][0] for i in range(len(train_ds))], 1).reshape(3, -1)\n",
        "stl10_mean, stl10_std = X.mean(1), X.std(1)\n",
        "print(stl10_mean, stl10_std)\n",
        "\n",
        "transform = transforms.Compose([transforms.RandomCrop(size=(96, 96), padding=3),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(stl10_mean, stl10_std),\n",
        "                                ])\n",
        "\n",
        "val_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize(stl10_mean, stl10_std)\n",
        "                                    ])\n",
        "\n",
        "pretrain_ds = STL10('.', split='unlabeled', folds=None, transform=transforms.ToTensor(), download=True)\n",
        "train_ds = STL10('.', split='train', folds=None, transform=transform, download=True)\n",
        "val_ds = STL10('.', split='test', transform=val_transform, download=True)\n",
        "\n",
        "pretrain_dl = DataLoader(pretrain_ds, batch_size=batch_size, shuffle=True)\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "log = Logger()\n",
        "\n",
        "network = DAE(pretrain=True)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "opt = optim.Adam(network.parameters(), lr=lr)\n",
        "\n",
        "# pre-training\n",
        "torch.cuda.init()\n",
        "\n",
        "for epoch in range(pretrain_epochs):\n",
        "    epoch_loss = []\n",
        "\n",
        "    network.train()\n",
        "    for batch in pretrain_dl:\n",
        "        img, cl = batch\n",
        "\n",
        "        noizy_img = img + noise_factor * torch.randn(img.shape)\n",
        "\n",
        "        noizy_img = np.clip(noizy_img, 0, 1)\n",
        "        opt.zero_grad()\n",
        "\n",
        "        output = network(noizy_img)\n",
        "\n",
        "        loss = loss_fn(output, noizy_img)\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        epoch_loss.append(loss.cpu().detach())\n",
        "\n",
        "    epoch_loss = torch.stack(epoch_loss).mean()\n",
        "    print(f' TRAIN | Epoch {epoch} - Loss: {epoch_loss:.6f}')\n",
        "\n",
        "\n",
        "    torch.save(network.state_dict(), './drive/MyDrive/pretrain.pt')\n",
        "\n",
        "nnt = DAE(pretrain=False)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "trainOpt = optim.Adam(nnt.parameters(), lr=lr)\n",
        "\n",
        "nnt.load_state_dict(torch.load('./drive/MyDrive/pretrain.pt'), strict=False)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    e_acc, e_loss = train(train_dl,nnt,loss_fn,trainOpt)\n",
        "    \n",
        "    log.write(f' TRAIN | Epoch {epoch} - Loss: {e_loss:.6f} | Acc: {e_acc:.4f}')\n",
        "    print(f' TRAIN | Epoch {epoch} - Loss: {e_loss:.6f} | Acc: {e_acc:.4f}')\n",
        "\n",
        "\n",
        "    val_loss, val_acc = validate(val_dl,nnt,loss_fn)\n",
        "    \n",
        "    log.write(f' VAL   | Epoch {epoch} - Loss: {val_loss:.6f} | Acc: {val_acc:.4f}')\n",
        "    print(f' VAL   | Epoch {epoch} - Loss: {val_loss:.6f} | Acc: {val_acc:.4f}')\n",
        "\n",
        "log.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "tensor([0.4467, 0.4398, 0.4066]) tensor([0.2603, 0.2566, 0.2713])\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            " TRAIN | Epoch 0 - Loss: 1.640294 | Acc: 0.3915\n",
            " VAL   | Epoch 0 - Loss: 1.377521 | Acc: 0.4890\n",
            " TRAIN | Epoch 1 - Loss: 1.328106 | Acc: 0.5064\n",
            " VAL   | Epoch 1 - Loss: 1.321580 | Acc: 0.5215\n",
            " TRAIN | Epoch 2 - Loss: 1.207790 | Acc: 0.5611\n",
            " VAL   | Epoch 2 - Loss: 1.187133 | Acc: 0.5729\n",
            " TRAIN | Epoch 3 - Loss: 1.119371 | Acc: 0.5862\n",
            " VAL   | Epoch 3 - Loss: 1.187411 | Acc: 0.5685\n",
            " TRAIN | Epoch 4 - Loss: 1.060973 | Acc: 0.6186\n",
            " VAL   | Epoch 4 - Loss: 1.147045 | Acc: 0.5789\n",
            " TRAIN | Epoch 5 - Loss: 1.036159 | Acc: 0.6304\n",
            " VAL   | Epoch 5 - Loss: 1.300034 | Acc: 0.5700\n",
            " TRAIN | Epoch 6 - Loss: 1.003615 | Acc: 0.6320\n",
            " VAL   | Epoch 6 - Loss: 1.096066 | Acc: 0.6015\n",
            " TRAIN | Epoch 7 - Loss: 0.936484 | Acc: 0.6606\n",
            " VAL   | Epoch 7 - Loss: 1.096683 | Acc: 0.6074\n",
            " TRAIN | Epoch 8 - Loss: 0.898440 | Acc: 0.6835\n",
            " VAL   | Epoch 8 - Loss: 1.234113 | Acc: 0.6064\n",
            " TRAIN | Epoch 9 - Loss: 0.862782 | Acc: 0.6919\n",
            " VAL   | Epoch 9 - Loss: 1.071374 | Acc: 0.6256\n",
            " TRAIN | Epoch 10 - Loss: 0.844233 | Acc: 0.6875\n",
            " VAL   | Epoch 10 - Loss: 1.275288 | Acc: 0.5914\n",
            " TRAIN | Epoch 11 - Loss: 0.789699 | Acc: 0.7116\n",
            " VAL   | Epoch 11 - Loss: 1.078518 | Acc: 0.6256\n",
            " TRAIN | Epoch 12 - Loss: 0.762762 | Acc: 0.7257\n",
            " VAL   | Epoch 12 - Loss: 1.035229 | Acc: 0.6420\n",
            " TRAIN | Epoch 13 - Loss: 0.740810 | Acc: 0.7321\n",
            " VAL   | Epoch 13 - Loss: 1.133314 | Acc: 0.6183\n",
            " TRAIN | Epoch 14 - Loss: 0.733109 | Acc: 0.7317\n",
            " VAL   | Epoch 14 - Loss: 1.136526 | Acc: 0.6406\n",
            " TRAIN | Epoch 15 - Loss: 0.686323 | Acc: 0.7484\n",
            " VAL   | Epoch 15 - Loss: 1.281064 | Acc: 0.6265\n",
            " TRAIN | Epoch 16 - Loss: 0.656867 | Acc: 0.7619\n",
            " VAL   | Epoch 16 - Loss: 1.245477 | Acc: 0.6267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btWkx8Zrez8E"
      },
      "source": [
        " # NO pretraining\n",
        " TRAIN | Epoch 0 - Loss: 1.993355 | Acc: 0.3107\n",
        " VAL   | Epoch 0 - Loss: 1.561598 | Acc: 0.4215\n",
        " TRAIN | Epoch 1 - Loss: 1.541898 | Acc: 0.4417\n",
        " VAL   | Epoch 1 - Loss: 1.374716 | Acc: 0.4886\n",
        " TRAIN | Epoch 2 - Loss: 1.365247 | Acc: 0.5004\n",
        " VAL   | Epoch 2 - Loss: 1.319850 | Acc: 0.5107\n",
        " TRAIN | Epoch 3 - Loss: 1.303779 | Acc: 0.5198\n",
        " VAL   | Epoch 3 - Loss: 1.335786 | Acc: 0.5071\n",
        " TRAIN | Epoch 4 - Loss: 1.226758 | Acc: 0.5509\n",
        " VAL   | Epoch 4 - Loss: 1.214242 | Acc: 0.5595\n",
        " TRAIN | Epoch 5 - Loss: 1.136996 | Acc: 0.5807"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPF9v6Ktw7-4"
      },
      "source": [
        "#pretraining itself\n",
        "\n",
        " TRAIN | Epoch 0 - Loss: 0.152718\n",
        " TRAIN | Epoch 1 - Loss: 0.067189\n",
        " TRAIN | Epoch 2 - Loss: 0.066569\n",
        " TRAIN | Epoch 3 - Loss: 0.066246"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhquOMWrCqhf"
      },
      "source": [
        "#classification with pretraining\n",
        " TRAIN | Epoch 0 - Loss: 1.640294 | Acc: 0.3915\n",
        " VAL   | Epoch 0 - Loss: 1.377521 | Acc: 0.4890\n",
        " TRAIN | Epoch 1 - Loss: 1.328106 | Acc: 0.5064\n",
        " VAL   | Epoch 1 - Loss: 1.321580 | Acc: 0.5215\n",
        " TRAIN | Epoch 2 - Loss: 1.207790 | Acc: 0.5611\n",
        " VAL   | Epoch 2 - Loss: 1.187133 | Acc: 0.5729\n",
        " TRAIN | Epoch 3 - Loss: 1.119371 | Acc: 0.5862\n",
        " VAL   | Epoch 3 - Loss: 1.187411 | Acc: 0.5685\n",
        " TRAIN | Epoch 4 - Loss: 1.060973 | Acc: 0.6186\n",
        " VAL   | Epoch 4 - Loss: 1.147045 | Acc: 0.5789\n",
        " TRAIN | Epoch 5 - Loss: 1.036159 | Acc: 0.6304\n",
        " VAL   | Epoch 5 - Loss: 1.300034 | Acc: 0.5700\n",
        " TRAIN | Epoch 6 - Loss: 1.003615 | Acc: 0.6320\n",
        " VAL   | Epoch 6 - Loss: 1.096066 | Acc: 0.6015\n",
        " TRAIN | Epoch 7 - Loss: 0.936484 | Acc: 0.6606\n",
        " VAL   | Epoch 7 - Loss: 1.096683 | Acc: 0.6074\n",
        " TRAIN | Epoch 8 - Loss: 0.898440 | Acc: 0.6835\n",
        " VAL   | Epoch 8 - Loss: 1.234113 | Acc: 0.6064\n",
        " TRAIN | Epoch 9 - Loss: 0.862782 | Acc: 0.6919\n",
        " VAL   | Epoch 9 - Loss: 1.071374 | Acc: 0.6256\n",
        " TRAIN | Epoch 10 - Loss: 0.844233 | Acc: 0.6875\n",
        " VAL   | Epoch 10 - Loss: 1.275288 | Acc: 0.5914\n",
        " TRAIN | Epoch 11 - Loss: 0.789699 | Acc: 0.7116\n",
        " VAL   | Epoch 11 - Loss: 1.078518 | Acc: 0.6256\n",
        " TRAIN | Epoch 12 - Loss: 0.762762 | Acc: 0.7257\n",
        " VAL   | Epoch 12 - Loss: 1.035229 | Acc: 0.6420\n",
        " TRAIN | Epoch 13 - Loss: 0.740810 | Acc: 0.7321\n",
        " VAL   | Epoch 13 - Loss: 1.133314 | Acc: 0.6183\n",
        " TRAIN | Epoch 14 - Loss: 0.733109 | Acc: 0.7317\n",
        " VAL   | Epoch 14 - Loss: 1.136526 | Acc: 0.6406\n",
        " TRAIN | Epoch 15 - Loss: 0.686323 | Acc: 0.7484\n",
        " VAL   | Epoch 15 - Loss: 1.281064 | Acc: 0.6265\n",
        " TRAIN | Epoch 16 - Loss: 0.656867 | Acc: 0.7619\n",
        " VAL   | Epoch 16 - Loss: 1.245477 | Acc: 0.6267"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}