{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled20.ipynb",
      "provenance": [],
      "mount_file_id": "1DmvCL7BYAGS3f3CKFgNPlXmKtCgEyb0_",
      "authorship_tag": "ABX9TyNFqNy0bGySarIv1HSwSuch",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vpysmennyi/machinelearning-learning/blob/main/course_gpt2/Untitled20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbhb2SpjkOII"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK9c_tW0kPKu",
        "outputId": "f25f2be0-53e0-4467-a237-2631866cc7cf"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.data_parallel as dp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler, random_split\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# set to True when running in google colab\n",
        "in_colab = True\n",
        "\n",
        "\n",
        "class ArxivDataset(Dataset):\n",
        "    def __init__(self, data_list, tokenizer, max_length=768):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.attention_msk = []\n",
        "\n",
        "        for dat in data_list:\n",
        "            tokenizer_encodings = tokenizer(dat, truncation=True, max_length=max_length, padding='max_length')\n",
        "\n",
        "            self.input_ids.append(torch.tensor(tokenizer_encodings['input_ids']))\n",
        "            self.attention_msk.append(torch.tensor(tokenizer_encodings['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attention_msk[idx]\n",
        "\n",
        "\n",
        "class ArxivAbstractGen():\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "        self.dataset = ArxivDataset(abstracts, tokenizer, max_length=config['encoding_max_length'])\n",
        "\n",
        "        self.epochs = config['num_epochs']\n",
        "        self.learning_rate = config['lr'] \n",
        "        self.warmup_steps = config['warmup_steps']\n",
        "        self.epsilon = config['epsilon']\n",
        "        self.batch_size = config['train_batch_size']\n",
        "\n",
        "        self.train_ds_len = 0\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.model\n",
        "\n",
        "    def prepare_data_loader(self, ds):\n",
        "        def _get_distributed_sampler(sliced_ds, shuffle=True):\n",
        "            sampler = DistributedSampler(\n",
        "                sliced_ds,\n",
        "                num_replicas=xm.xrt_world_size(),\n",
        "                rank=xm.get_ordinal(),\n",
        "                shuffle=shuffle)\n",
        "            return sampler\n",
        "\n",
        "        train_size = int(len(ds) * config['train_size_percent'] / 100)\n",
        "        val_size = len(ds) - train_size\n",
        "\n",
        "        train_ds, val_ds = random_split(ds, [train_size, val_size])\n",
        "\n",
        "        self.train_ds_len = len(train_ds)\n",
        "\n",
        "        train_dl = DataLoader(train_ds,\n",
        "                              sampler=_get_distributed_sampler(train_ds, shuffle=True),\n",
        "                              batch_size=self.batch_size)\n",
        "        val_dl = DataLoader(val_ds,\n",
        "                            sampler=_get_distributed_sampler(val_ds, shuffle=False),\n",
        "                            batch_size=self.batch_size)\n",
        "        \n",
        "        return train_dl, val_dl\n",
        "\n",
        "    def model_save(self):\n",
        "        xm.master_print('saving the model')\n",
        "        self.model.to('cpu')\n",
        "        model_to_save = self.model.module if hasattr(self.model,\n",
        "                                                'module') else self.model  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(config['save_dir'])\n",
        "        tokenizer.save_pretrained(config['save_dir'])\n",
        "        xm.master_print('saved')\n",
        "\n",
        "    def print_train_stats(self, stats):\n",
        "        pd.set_option('precision', 2)\n",
        "        stat = pd.DataFrame(data=stats)\n",
        "\n",
        "        stat = stat.set_index('epoch')\n",
        "        xm.master_print(stat)\n",
        "\n",
        "    def generate_sample(self, top_k=50, max_l=200, top_p=0.92, num_seq=1):\n",
        "        xm.master_print('Generating sample:')\n",
        "        xm.rendezvous('generate smpl')\n",
        "        # moving to cpu for generation\n",
        "        self.model.to('cpu')\n",
        "        sample_output = self.model.generate(bos_token_id=random.randint(1, 30000),\n",
        "                                       do_sample=True,\n",
        "                                       top_k=top_k,\n",
        "                                       max_length=max_l,\n",
        "                                       top_p=top_p,\n",
        "                                       num_return_sequences=num_seq)\n",
        "        for i, sample in enumerate(sample_output):\n",
        "            xm.master_print(f'{i} : {self.tokenizer.decode(sample, skip_special_tokens=True)}')\n",
        "        \n",
        "    def run_training(self, tpu_dev):\n",
        "\n",
        "        # get data loader sets\n",
        "        train_dl, val_dl = self.prepare_data_loader(self.dataset)\n",
        "\n",
        "        self.model.to(tpu_dev)\n",
        "\n",
        "        opt = AdamW(self.model.parameters(), \n",
        "                    lr=self.learning_rate * xm.xrt_world_size(), \n",
        "                    eps=self.epsilon)\n",
        "\n",
        "        total_steps = int(self.train_ds_len / self.batch_size / xm.xrt_world_size() * self.epochs)\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(opt, \n",
        "                                                    num_warmup_steps=self.warmup_steps, \n",
        "                                                    num_training_steps=total_steps)\n",
        "\n",
        "        t0 = time.time()\n",
        "        xm.master_print(f'num_train_steps = {total_steps}, TPU cores={xm.xrt_world_size()}')\n",
        "\n",
        "        train_stats = []\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            t0_epoch = time.time()\n",
        "            \n",
        "            p_train_dl = pl.ParallelLoader(train_dl, [tpu_dev])\n",
        "            p_val_dl = pl.ParallelLoader(val_dl, [tpu_dev])\n",
        "\n",
        "            \n",
        "            #devices = xm.get_xla_supported_devices(max_devices=config['num_tpu_cores'])\n",
        "            #model = dp.DataParallel(self.model, device_ids=devices)\n",
        "\n",
        "            val_epoch_loss = []\n",
        "            epoch_loss = []\n",
        "\n",
        "            ############ TRAINING\n",
        "            self.model.train()\n",
        "            for step, batch in enumerate(p_train_dl.per_device_loader(tpu_dev)):\n",
        "\n",
        "                opt.zero_grad()\n",
        "\n",
        "                b_input_ids = batch[0].to(tpu_dev)\n",
        "                b_labels = batch[0].to(tpu_dev)\n",
        "                b_att_mask = batch[1].to(tpu_dev)\n",
        "\n",
        "                output = self.model(b_input_ids,\n",
        "                               labels=b_labels,\n",
        "                               attention_mask=b_att_mask,\n",
        "                               token_type_ids=None)\n",
        "\n",
        "                loss = output[0]\n",
        "                epoch_loss.append(loss.item())\n",
        "\n",
        "                #if step % 10 == 0:\n",
        "                #    xm.master_print(f'Step: {step}')\n",
        "\n",
        "                '''if config['mid_sample_enable']:\n",
        "                    if step % config['sample_every_steps'] == 0 and step > 0:\n",
        "                        model.eval()\n",
        "\n",
        "                        generate_sample(model, tokenizer, config['decode_top_k'], config['decode_max_length'],\n",
        "                                        config['decode_top_p'])\n",
        "                        # moving back to tpu to proceed with training\n",
        "                        model.to(tpu_dev)\n",
        "                        model.train()'''\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                # need to use this for parallelism\n",
        "                xm.optimizer_step(opt)\n",
        "                \n",
        "                scheduler.step()\n",
        "\n",
        "            train_time = format_time(time.time() - t0_epoch)\n",
        "            epoch_loss = np.array(epoch_loss).mean()\n",
        "\n",
        "            if epoch_loss:\n",
        "                xm.master_print(f'TRAIN | Epoch {epoch + 1} : Loss = {epoch_loss}. Elapsed: {train_time}')\n",
        "            else:\n",
        "                print('Epoch loss is empty')\n",
        "\n",
        "            ############ VALIDATION\n",
        "            self.model.eval()\n",
        "\n",
        "            val_t0 = time.time()\n",
        "\n",
        "            for val_step, batch in enumerate(p_val_dl.per_device_loader(tpu_dev)):\n",
        "                #xm.master_print(f'Val step {val_step}')\n",
        "                b_input_ids = batch[0].to(tpu_dev)\n",
        "                b_labels = batch[0].to(tpu_dev)\n",
        "                b_att_mask = batch[1].to(tpu_dev)\n",
        "\n",
        "                output = self.model(b_input_ids,\n",
        "                               labels=b_labels,\n",
        "                               attention_mask=b_att_mask,\n",
        "                               token_type_ids=None)\n",
        "\n",
        "                loss = output[0]\n",
        "                val_epoch_loss.append(loss.item())\n",
        "\n",
        "            val_time = format_time(time.time() - val_t0)\n",
        "            val_epoch_loss = np.array(val_epoch_loss).mean()\n",
        "\n",
        "            if val_epoch_loss:\n",
        "                xm.master_print(f'VAL | Epoch {epoch + 1} : Loss = {val_epoch_loss}. Elapsed: {val_time}')\n",
        "            else:\n",
        "                print('Empty val_epoch_loss')\n",
        "\n",
        "            # Record all statistics from this epoch.\n",
        "            train_stats.append(\n",
        "                {\n",
        "                    'epoch': epoch + 1,\n",
        "                    'Training Loss': epoch_loss,\n",
        "                    'Valid. Loss': val_epoch_loss,\n",
        "                    'Training Time': train_time,\n",
        "                    'Validation Time': val_time\n",
        "                }\n",
        "            )\n",
        "\n",
        "            xm.save(self.model.state_dict(), config['save_dir'] + 'model_e.pt')\n",
        "\n",
        "        xm.master_print(f'Total elapsed: {format_time(time.time() - t0)}')\n",
        "        self.print_train_stats(train_stats)\n",
        "        xm.rendezvous('leave')\n",
        "\n",
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round(elapsed))))\n",
        "\n",
        "def _mp_fn(rank, flags, trainer_obj):\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "\n",
        "    # initializing TPU device\n",
        "    device = xm.xla_device()\n",
        "    xm.rendezvous('init')\n",
        "    trainer_obj.run_training(device)\n",
        "\n",
        "# reading config\n",
        "conf_file = './config.json'\n",
        "\n",
        "if in_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    conf_file = 'drive/MyDrive/ArxivDS/colab_config.json'\n",
        "\n",
        "with open(conf_file) as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "print('Execution configuration:')\n",
        "for c in config:\n",
        "    print(f'{c}' + ' '*(30-len(c)) + f'{config[c]}')\n",
        "\n",
        "#os.environ[\"XRT_TPU_CONFIG\"] = \"tpu_worker;0;\" + config['tpu_ip_address'] + \":8470\"\n",
        "\n",
        "#reading dataset\n",
        "df = pd.read_json(config['datafile'], lines=True, nrows=80000)\n",
        "abstracts = df.abstract\n",
        "\n",
        "#loading GPT2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<BOS>', eos_token='<EOS>', pad_token='<PAD>')\n",
        "gpt2_config = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "mdl = GPT2LMHeadModel.from_pretrained('gpt2', config=gpt2_config)\n",
        "mdl.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "trainer = ArxivAbstractGen(mdl, tokenizer)\n",
        "\n",
        "xmp.spawn(_mp_fn, args=(config, trainer), nprocs=config['num_tpu_cores'], start_method='fork')\n",
        "\n",
        "trainer.model_save()\n",
        "\n",
        "#generating samples with a tuned model\n",
        "trainer.generate_sample(top_k=config['decode_top_k'], \n",
        "                        max_l=config['decode_max_length'], \n",
        "                        top_p=config['decode_top_p'],\n",
        "                        num_seq=config['decode_num_test_samples'])\n",
        "\n",
        "#export XRT_TPU_CONFIG=\"tpu_worker;0;$TPU_IP_ADDRESS:8470\"\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Execution configuration:\n",
            "encoding_max_length           256\n",
            "datafile                      drive/MyDrive/ArxivDS/arxiv-abstracts.json\n",
            "save_dir                      drive/MyDrive/ArxivDS/model_save/\n",
            "num_tpu_cores                 1\n",
            "decode_top_k                  50\n",
            "decode_max_length             200\n",
            "decode_top_p                  0.92\n",
            "decode_num_test_samples       3\n",
            "train_batch_size              2\n",
            "train_size_percent            95\n",
            "num_epochs                    5\n",
            "lr                            5e-05\n",
            "warmup_steps                  100\n",
            "epsilon                       1e-08\n",
            "mid_sample_enable             False\n",
            "sample_every_steps            100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_train_steps = 190000, TPU cores=1\n",
            "TRAIN | Epoch 1 : Loss = 2.160737574042066. Elapsed: 1:31:42\n",
            "VAL | Epoch 1 : Loss = 1.9624112636968494. Elapsed: 0:01:57\n",
            "TRAIN | Epoch 2 : Loss = 1.9302689365407355. Elapsed: 1:30:48\n",
            "VAL | Epoch 2 : Loss = 1.9046180317923427. Elapsed: 0:01:59\n",
            "TRAIN | Epoch 3 : Loss = 1.841948399081434. Elapsed: 1:34:02\n",
            "VAL | Epoch 3 : Loss = 1.8786731933131815. Elapsed: 0:02:00\n",
            "TRAIN | Epoch 4 : Loss = 1.7830171864111173. Elapsed: 1:30:36\n",
            "VAL | Epoch 4 : Loss = 1.8648985799849034. Elapsed: 0:01:58\n",
            "TRAIN | Epoch 5 : Loss = 1.743164057972792. Elapsed: 1:30:17\n",
            "VAL | Epoch 5 : Loss = 1.8573311190456152. Elapsed: 0:01:58\n",
            "Total elapsed: 7:47:55\n",
            "       Training Loss  Valid. Loss Training Time Validation Time\n",
            "epoch                                                          \n",
            "1               2.16         1.96       1:31:42         0:01:57\n",
            "2               1.93         1.90       1:30:48         0:01:59\n",
            "3               1.84         1.88       1:34:02         0:02:00\n",
            "4               1.78         1.86       1:30:36         0:01:58\n",
            "5               1.74         1.86       1:30:17         0:01:58\n",
            "saving the model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "saved\n",
            "Generating sample:\n",
            "0 :  planet   This is a comment on a review article by D. Cazenave, P. Geitel, J. A. Kulkarni, A. J. Shor and B. I. Bohm.  enger's papers ``Statistical analysis of the electromagnetic energy in a nonrelativistic plasma'' [arXiv:0808.4517]  enger's comments ``The case for the existence of Lorentz-invariant and Lorentz-invariant energy scales for the total electromagnetic energy in relativistic plasmas'' [arXiv:0808.5377]  enger's comments ``Nonrelativistic plasma models of the electromagnetic energy spectrum: implications of new physics models''\n",
            "1 :  planet   We describe the recent development of the 3D magnetohydrodynamics code HADES for studying the evolution of magnetically-driven, rotating accretion disks. The code calculates stellar magnetospheres that are in a spin-down state, rotating rapidly, and which are highly magnetically-driven (R = 10^9 K and 3 degrees). As a first application, we calculate the vertical and radial distributions of accreting disks, in the presence of their spin-down or spin-up conditions. We show that at the stellar poles the vertical and radial distributions of accretion disks are dominated by disk-jet components. Thus the horizontal and vertical distributions of these accreting disks follow a different kinematical and structural phase than those of normal accreting disks. The disk-jet components contribute substantially to the rotational magnetization, and this effect can be enhanced when the disk inclination is angle-resolved. We also show how to modify the disk dynamics to\n",
            "2 :  planet   Theoretical predictions of the neutrino mass hierarchy for the first few neutrino oscillation period after leptogenesis are presented. Numerical simulations of an isotropic (0.1,0.1) neutrino oscillation with a fixed number of particles and a fixed effective temperature of 0.01 $\\mu$B are performed. The numerical results show that if the initial neutrino mass hierarchy for the first one year is of order $\\mu$B, the second one year should be much larger than the first one year. If the initial neutrino mass hierarchy has a constant value $\\mu_{13}=2\\nu_\\mu^2$, then the neutrino mass hierarchy must be of order $\\mu_{13}^{+2}$ for some range of $\\mu \\ll 1$ and $\\mu \\ll 6$ times larger than $\\mu_{11}=1.3 \\nu_\\mu^2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtn31aM80Xq_"
      },
      "source": [
        "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
        "Execution configuration:\n",
        "encoding_max_length           256\n",
        "datafile                      drive/MyDrive/ArxivDS/arxiv-abstracts.json\n",
        "save_dir                      drive/MyDrive/ArxivDS/model_save/\n",
        "num_tpu_cores                 1\n",
        "decode_top_k                  50\n",
        "decode_max_length             200\n",
        "decode_top_p                  0.92\n",
        "decode_num_test_samples       3\n",
        "train_batch_size              2\n",
        "train_size_percent            95\n",
        "num_epochs                    5\n",
        "lr                            5e-05\n",
        "warmup_steps                  100\n",
        "epsilon                       1e-08\n",
        "mid_sample_enable             False\n",
        "sample_every_steps            100\n",
        "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
        "num_train_steps = 190000, TPU cores=1\n",
        "TRAIN | Epoch 1 : Loss = 2.160737574042066. Elapsed: 1:31:42\n",
        "VAL | Epoch 1 : Loss = 1.9624112636968494. Elapsed: 0:01:57\n",
        "TRAIN | Epoch 2 : Loss = 1.9302689365407355. Elapsed: 1:30:48\n",
        "VAL | Epoch 2 : Loss = 1.9046180317923427. Elapsed: 0:01:59\n",
        "TRAIN | Epoch 3 : Loss = 1.841948399081434. Elapsed: 1:34:02\n",
        "VAL | Epoch 3 : Loss = 1.8786731933131815. Elapsed: 0:02:00\n",
        "TRAIN | Epoch 4 : Loss = 1.7830171864111173. Elapsed: 1:30:36\n",
        "VAL | Epoch 4 : Loss = 1.8648985799849034. Elapsed: 0:01:58\n",
        "TRAIN | Epoch 5 : Loss = 1.743164057972792. Elapsed: 1:30:17\n",
        "VAL | Epoch 5 : Loss = 1.8573311190456152. Elapsed: 0:01:58\n",
        "Total elapsed: 7:47:55\n",
        "       Training Loss  Valid. Loss Training Time Validation Time\n",
        "epoch                                                          \n",
        "1               2.16         1.96       1:31:42         0:01:57\n",
        "2               1.93         1.90       1:30:48         0:01:59\n",
        "3               1.84         1.88       1:34:02         0:02:00\n",
        "4               1.78         1.86       1:30:36         0:01:58\n",
        "5               1.74         1.86       1:30:17         0:01:58\n",
        "saving the model\n",
        "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
        "saved\n",
        "Generating sample:\n",
        "0 :  planet   This is a comment on a review article by D. Cazenave, P. Geitel, J. A. Kulkarni, A. J. Shor and B. I. Bohm.  enger's papers ``Statistical analysis of the electromagnetic energy in a nonrelativistic plasma'' [arXiv:0808.4517]  enger's comments ``The case for the existence of Lorentz-invariant and Lorentz-invariant energy scales for the total electromagnetic energy in relativistic plasmas'' [arXiv:0808.5377]  enger's comments ``Nonrelativistic plasma models of the electromagnetic energy spectrum: implications of new physics models''\n",
        "1 :  planet   We describe the recent development of the 3D magnetohydrodynamics code HADES for studying the evolution of magnetically-driven, rotating accretion disks. The code calculates stellar magnetospheres that are in a spin-down state, rotating rapidly, and which are highly magnetically-driven (R = 10^9 K and 3 degrees). As a first application, we calculate the vertical and radial distributions of accreting disks, in the presence of their spin-down or spin-up conditions. We show that at the stellar poles the vertical and radial distributions of accretion disks are dominated by disk-jet components. Thus the horizontal and vertical distributions of these accreting disks follow a different kinematical and structural phase than those of normal accreting disks. The disk-jet components contribute substantially to the rotational magnetization, and this effect can be enhanced when the disk inclination is angle-resolved. We also show how to modify the disk dynamics to\n",
        "2 :  planet   Theoretical predictions of the neutrino mass hierarchy for the first few neutrino oscillation period after leptogenesis are presented. Numerical simulations of an isotropic (0.1,0.1) neutrino oscillation with a fixed number of particles and a fixed effective temperature of 0.01 $\\mu$B are performed. The numerical results show that if the initial neutrino mass hierarchy for the first one year is of order $\\mu$B, the second one year should be much larger than the first one year. If the initial neutrino mass hierarchy has a constant value $\\mu_{13}=2\\nu_\\mu^2$, then the neutrino mass hierarchy must be of order $\\mu_{13}^{+2}$ for some range of $\\mu \\ll 1$ and $\\mu \\ll 6$ times larger than $\\mu_{11}=1.3 \\nu_\\mu^2"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}